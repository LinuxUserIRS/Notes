Considere um conjunto de dados que queremos classificar em 2 classes. Podemos fazer isso linearmente com vários hiperplanos diferentes. Um hiperplano é definido por x.w+b, onde x é o dado a ser clasificado, w é o peso e b é o bias. Podemos ter hiperplanos com diferentes pesos e bias que separam corretamente os dados. Entretanto, como escolher o hiperplano ótimo?
Primeiramente, precisamos definir o hiperplano ótimo. **Hiperplano ótimo** é o que é equidistante das classes e possui a maior margem de separação entre elas possível. Os pontos mais próximos do hiperplano ótimo são chamados **vetores suporte**. 
**Calculando o hiperplano ótimo:** Temos 2 hiperplanos, o superior e o inferior, cada um tocando os vetores suporte de cada uma de suas classes. A **margem**, ou seja, a separação entre esses hiperplanos é a diferença entre suas respectivas funções. Considerando que ambos os hiperplanos são paralelos, a margem é 2/W. Como queremos a maior margem possível, precisamos minimizar W.
Vimos em uma seção anterior (eu acho), que, quando temos dados que não são linearmente separáveis, é uma tática comum levá-los para uma dimensionalidade maior. Chamamos de phi a função que faz esse mapeamento. Mapear múltiplos valores em dimensões elevadas pode levar tempo e ser custoso, mas existe a **função kernel** para simplificar esse processo. A função kernel é uma função que é equivalente a um produto interno em um espaço de maior dimensionalidade. Kernels são úteis pois não requerem mapeamento explícito da função phi.
SVMs são muito práticos no mundo real, pois têm um bom desempenho empírico e fundamentação teórica forte.